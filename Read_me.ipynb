{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "I94_Immigration_Data\n",
    "#### Project Summary\n",
    "I94 immigration data comes from the National Travel and Tourism Office. The report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "\n",
    "My end solution will give an insight on how many international visitors arrive to US cities monthly each year.\n",
    "\n",
    "As a data engineer, I have been assigned to Build an ETL pipeline for a datalake hosted on github. To complete the project, will need to load data from github, process the data into analytics tables using Spark and writes them in parquet files in a separate analytics directory.The final transformed data in facts and dimensions will be pushed in to parquet files, where all analytical team will be interacting and trying to find out some insights that may effectively place resources in the US cities where there is high international visitors Arrivals.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project follows the follow steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 1: Scope the Project and Gather Data\n",
    "#### Step 1:\n",
    "\n",
    "#### Project Description\n",
    "\n",
    "In this project, I have applied what i have learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on github.\n",
    "Also I have used my best ability to load the data into parquet files in a separate analytics directory where the actual facts and dimensions data will be resided as my final target for the analytics team to use that for their effective analysis.\n",
    "\n",
    "\n",
    "To complete the project, i have loaded data from github, process the data into analytics tables using Spark, and and writes them in parquet files in a separate analytics directory. Final target destination is Immigration Warehouse Directory. \n",
    "\n",
    "#### Project Datasets\n",
    "\n",
    "The SAS file is located in the local workspace and I used that as source.\n",
    "\n",
    "* '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 2: Explore and Assess the Data\n",
    "#### Immigration Datasets\n",
    "\n",
    "immigration data is in a folder with the following path: ../../data/18-83510-I94-Data-2016/. There's a file for each month of the year.Each file has a three-letter abbreviation for the month name.\n",
    "\n",
    "#### Schema for Immigration Data\n",
    "\n",
    "* i94yr\n",
    "* i94mon\n",
    "* i94cit\n",
    "* i94res\n",
    "* i94port\n",
    "* arrdate\n",
    "* i94mode\n",
    "* i94addr\n",
    "* depdate\n",
    "* i94bir\n",
    "* i94visa\n",
    "* dtadfile\n",
    "* visapost\n",
    "* biryear\n",
    "* dtaddto\n",
    "* gender\n",
    "* insnum\n",
    "* airline\n",
    "* admnum\n",
    "* fltno\n",
    "* visatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 3: Define the Data Model\n",
    "\n",
    "#### Dimensional Modelling:\n",
    "A dimensional model is also commonly called a star schema.The core of the star schema model is built from fact tables and dimension tables. It consists, typically, of a large table of facts (known as a fact table), with a number of other tables surrounding it that contain descriptive data, called dimensions. \n",
    "\n",
    "#### Fact Table:\n",
    "The fact table contains numerical values of what you measure. Each fact table contains the keys to associated dimension tables. Fact tables have a large number of rows.The information in a fact table has characteristics. It is numerical and used to generate aggregates and summaries. All facts refer directly to the dimension keys. Fact table that is determined after carefull analysis which contains the information.\n",
    "\n",
    "#### Tables (Facts)\n",
    "Table Name: i94_immigration_data(fact)\n",
    "\n",
    "Column Names: \n",
    "* i94_admission_num\n",
    "* i94_arrival_date\n",
    "* visa_type\n",
    "* i94_date_admitted_until \n",
    "* i94_port_description\n",
    "* i94_addr_description \n",
    "* travel_city\n",
    "* residence\n",
    "* category \n",
    "* visitor_age \n",
    "* birth_year \n",
    "* visitor_gender\n",
    "* insurance_number\n",
    "\n",
    "\n",
    "#### Dimension Tables:\n",
    "The dimension tables contain descriptive information about the numerical values in the fact table. \n",
    "\n",
    "#### Tables ( Dimensions )\n",
    "\n",
    "Table Name:Visitor_Arrival_Modes  (mode of arrrival Dimension)\n",
    "Column Names: i94mode, category, airile, fltno\n",
    "\n",
    "Table Name: visitor_details (visitor detail Dimension)\n",
    "Column Names: i94cit,travel_city,i94res,residence,visitor_age,visitor_gender\n",
    "\n",
    "Table Name: visitor_i94_codes (Visitor I94 port of entry codes Dimension)\n",
    "Column Names: i94_addr,i94_addr_description,i94_port,i94_port_description\n",
    "\n",
    "Table Name: visitor_visa (Visitor Visa Dimension)\n",
    "Column Names: i94_visa,i94_visa_description,visa_type\n",
    "\n",
    "Table Name: visitor_times (Visitor Time Dimension)\n",
    "Column Names: i94_admission_num,i94_year,i94_month,i94_arrival_date,i94_departure_date,i94_date_added_on_file,i94_date_admitted_until\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 4: Run ETL to Model the Data\n",
    "#### ETL Pipeline\n",
    "ETL pipeline consists of three steps.\n",
    "* Extraction \n",
    "* Transformation\n",
    "* Load \n",
    "\n",
    "The data is extracted from extract function which returns the dataframe. This dataframe is the input for the i94_df function to get the appropriate transformations and then load them as parquet files for analytical team to access them for further analysis.\n",
    "\n",
    "#### Database creation and loading Approach: \n",
    "Build an ETL pipeline to extract data from the SAS file from github and push the data into necessary dimensions and fact tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Step 5: Complete Project Write Up\n",
    "\n",
    "Data Cleaning: \n",
    "\n",
    "   1) Read all the Data from github using pyspark(spark.read.json) API.\n",
    "   2) Extracting Data and doing the Spark Data Processing(In-Memory Computation) to create the Individual fact and Dimension tables.\n",
    "   3) Identified and removed duplicates(Getting Distinct Data).\n",
    "   5) Extracted the time related information from the arrdate column into a visitor time table.\n",
    "   6) Each of the five tables are written to parquet files in a separate analytics directory. Each table has its own folder within the directory. Visitor_Arrival_Modes Table files are       partitioned by i94 mode. visitor_details table files are partitioned by travel city and gender. visitor_i94_codes table files are partitioned by i94 address and i94 port.               visitor_visa table files are partitioned by i94 visa. visitor_times table files are partitioned by i94 year and month. Final I94 Immigration Data table files are partioned by           visa type. \n",
    "****************************************************************************************************************************\n",
    "Python Scripts:\n",
    "\n",
    "\n",
    "1) etl.py performs the following tasks mentioned below:\n",
    "   1) The main Function connects and Creates a Spark Session and executes visitor_mod_arrival,visitor_table,visitor_i94_codes,visitor_visa,visitor_time and i94_immigration_data               Functions using PySpark and extracts all the table columns and writes the data into parquet files(Columnsar Storage) in the Immigration Ware-House Directory.\n",
    "************************************************************************************************************************   \n",
    "Jupyter Notebook files:\n",
    "\n",
    "\n",
    "   1) etl.ipynb is used to work on the project exection process initially.\n",
    "   4) Run.ipynb will run \"Python etl.py\"(etl process).\n",
    "***************************************************************************************************************************\n",
    "Execution Order:\n",
    "\n",
    "\n",
    "    1) %run -i etl.py\n",
    "****************************************************************************************************************************\n",
    "Data Quality Checks:\n",
    "\n",
    "    Queries to check Using Spark sql:\n",
    "    \n",
    "    Inital Main Data set count: \n",
    "    df_spark.count()   ------3096313 records\n",
    "    \n",
    "    Dimension Tables:\n",
    "     1) select count(*) from Visitor_Arrival_Modes; ----12020 Records\n",
    "     2) select count(*) from visitor_details;----104509 Records\n",
    "     3) select count(*) from visitor_i94_codes;----6638 records\n",
    "     4) select count(*) from visitor_visa;----17 records\n",
    "     5) select count(*) from visitor_times;----3096241 records\n",
    "     \n",
    "    Fact Tables:\n",
    "     1) select count(*) from i94_immigration_data;---2618419 records\n",
    "****************************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis:\n",
    "\n",
    "\n",
    "These are some of the assumed analysis as per my knowledge so far, even though these may not be the carved stone scenarios, the point is data is available for the analysts to use to improve Immigration staff to add additional resources in the most busiest US cities.\n",
    "\n",
    "Analyis 1: to find top US Port Of Entry Cities with most number of I94 Admissions\n",
    "output: Please refer to the Analysis1.PNG\n",
    "\n",
    "\n",
    "select count(i94_admission_num) total_admissions,i94_port_description \n",
    "from i94_immigration_data \n",
    "group by i94_port_description \n",
    "order by total_admissions desc;\n",
    "\n",
    "\n",
    "Analysis 2: to find cities and their residences of most number of male I94 admissions who are below 30.\n",
    "output: Please refer to the  Analysis2.PNG\n",
    "\n",
    "\n",
    "select count(i94_admission_num) total_admissions,travel_city,residence \n",
    "from i94_immigration_data \n",
    "where visitor_gender = \"M\" \n",
    "and visitor_age < 30 \n",
    "group by travel_city,residence \n",
    "order by total_admissions desc;\n",
    "\n",
    "\n",
    "Aim: to find most number of I94 Admissions who are living in US States and do not have an Admit untill date.\n",
    "output: Please refer to the  Analysis3.PNG\n",
    "\n",
    "\n",
    "\n",
    "select count(i94_admission_num) total_admissions,i94_addr_description \n",
    "from i94_immigration_data \n",
    "where i94_date_admitted_until is null \n",
    "group by i94_addr_description \n",
    "order by total_admissions desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
